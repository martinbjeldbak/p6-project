\section{Complexity}\label{sec:complexity}
For any practical application of \dia{}, it is important to compare its complexity to that of the two widely used diversity measures, namely Hamming distance and Fitness-based diversity measure.

\subsection{Complexity of \dia}
The computations associated with \dia{} is split into two parts. The first part distributes neural networks into a number of buckets (species) for each random input. These buckets are used in the second part where an SDI is calculated for each random input. 
The first part of computations is carried out as follows:
For each random input $\ran \in \ranset$, every neural network $\ind \in \indset$ must calculate an output for each output neuron $\nnout \in \nnoutset$ based on \ran. Each output neuron \nnout{} is now considered once to calculate the particular species \ind{} belongs to.
The complexity of the first part is thus $\bigO{\ransetl \cdot \indsetl(\timet+\nnoutsetl)}$, where \timet{} is the time required to calculate the output of a neural network given any input. When calculating the output of a neural network, every neuron in the network must be considered. Therefore, \timet{} is always greater than \nnoutsetl, and we get the complexity $\bigO{\ransetl \cdot \indsetl(\timet)}$ for the first part.
In the second part, we note that SDI considers every bucket once for every random input, and hence this part has complexity
$\bigO{\ransetl\cdot\bucketsetl}$, where \bucketset{} is the set of non-empty buckets. Since the each neural network is put in only one bucket, $\indsetl$ is an upper bound of $\bucketsetl$, yielding the complexity $\bigO{\ransetl\cdot\indsetl}$ of the second part.
Summing up, the total complexity of \dia{} is $\bigO{\ransetl \cdot \indsetl \cdot \timet + \ransetl \cdot \indsetl}$, which is $\bigO{\ransetl\cdot\indsetl\cdot\timet}$

\subsection{Complexity of Hamming distance}
The Hamming distance of a set of neural networks \indset{} is determined
by calculating the Hamming distance $h_{ij}$ between any two neural networks $\ind_i, \ind_j \in \indset$.
The time required to calculate $h_{ij}$ is \bigO{\bitstringl}, hence we get a total complexity of
\bigO{\indsetl^2 \cdot \bitstringl}.

\subsection{Complexity of Fitness-based diversity measure}
In a typical GA, the fitness of each individual must calculated to ensure that more fit individuals have a higher chance to be selected for the creation of offspring.
Therefore, the calculation of fitness values will not be considered a part of the complexity measure.
To calculate the number of distinct individuals, we can use a hash set data structure. If we assume it unlikely to have a clash between hash values and choose to ignore this, we can achieve a complexity of \bigO{\indsetl} for the fitness-based diversity measures.

\subsection{Comparison of complexities}
The fitness-based diversity measure is definitely the cheapest to calculate.
We can ease the comparison of the complexity $\bigO{\ransetl\cdot\indsetl\cdot\timet}$ of \dia{} to the complexity \bigO{\indsetl^2 \cdot \bitstringl} of Hamming distance, by removing the common factor \indsetl.
Additionally , we can make an assumption about how the length of a bit string \bitstringl{} is related to the value \timet, which is the work required to calculate the output of a neural network given any input.
Let $|E|$ denote the number of edges in a neural network.
Since each weight must be considered when the output of a neural network is calculated, $\timet${} is at least $|E|$.
Additionally, some constant time will be spend applying the activation function.
In the worst case, each neuron has only a single input edge. In that case, the activation function will be applied
$|E|$ times, and just contribute as a constant factor to $\timet$. Therefore, we have that $\timet = \bigT{|E|}$.
If we assume that $|E| = \bigT{\bitstringl}$, we also get that $\timet = \bigT{\bitstringl}$.
This assumption is reasonable because the number of edges in a neural network is an upper bound of the number of neurons.
With this assumption, we can simplify the complexity comparison of \dia{} and Hamming distance, to a comparison of the complexities $\bigO{\ransetl}$ and \bigO{\indsetl}, respectively.
In this manner, we can conclude, that the complexity of \dia{} and Hamming distance is asymptotically the same, whenever
$\ransetl = c \cdot \indsetl$. That is, whenever the number of random inputs chosen for \dia{} is just a constant times the population size.