\section{Introduction}
For many problems in computer science, a greedy strategy will not always lead to the best result. A greedy strategy chooses the next step by considering how promising each step is at that given moment. As an example, if a mountain climber wants to reach the Mount Everest, a greedy strategy would be to move in any direction he think is the best, as long as he climbs upwards and never downwards. Using this strategy, he is definitely able to climb up a peak, but once he reaches one, he might discover an even greater peak he could not see before. Unfortunately, his strategy is to never back down, so now he is stuck at what we call a local optimum. What he wanted was the global optimum of Mount Everest.
 
Genetic algorithms (GAs) can be used to search for a global optimum in many types of problems, such as optimization problems. A GA manages a number of individuals, which constitute a population. Each of the individuals represents a candidate solution to the same problem. Neural networks, among other data structures, can chosen as individuals. A neural network takes a number of inputs, do some internal calculations, and then yield a number of outputs.
The neural network as its whole can be interpreted as a solution to the problem.
For instance, for the particular problem of adding two integers, the neural network can take two integers $s$ and $t$ as input, and output a single value, which ideally should equal $s + t$.

Most often, the individuals in a GA do not solve the problem in question adequately.
However, different individuals may have good solutions to different aspects of the problem.
Individuals maintained by the GA procreate to form offspring that combine the best traits from both its parents. Intuitively, the combination of these traits constitutes a better solution to the given problem. Each individual has a fitness value, which indicates how adequately it solves the problem in question.

To identify a constructive combination of traits, a diverse set of individuals is required. If diversity is not maintained, only a local optimum of the population's available traits will be explored, and a global optimum might never be found~\cite{ursem2002diversity,Darwen00doesextra}.

We believe it is essential that a diversity measure reflects the difference in traits among individuals. By concentrating on neural networks as individuals, we develop a new diversity measure that, as we will see, is not affected by pitfalls in traditional diversity measures.

Since a \emph{trait} is a rather vague term, we introduce a clear definition of traits among neural networks: ``\emph{two neural networks have different traits, if they for some input produce different outputs}''. We now argue why we think that neither fitness-based nor genotypic diversity measures catch a diversity among traits, or \emph{trait diversity}.

Consider two individuals who try to find the highest peak on an elevation map, where the fitness of each individual is based on the height of the peak they return. The two individuals might have completely different strategies, causing them to get stuck on two different peaks. If both peaks happen to have the same height, the two individuals will have the same fitness, and hence a fitness-based diversity measure will return a low diversity, even though the two individuals have different traits.

Two individuals of different genotypes can have the same behaviour, which means they yield the same output on any input. An example of such two neural networks is shown in \cref{fig:entire-eqnetwork}. No matter what input they receive, their output will always be the same. They are genotypically diverse, because their genetic structure is different, but they are not trait diverse, because they produce the same output. The genetic structure could be represented as bit string as shown in \cref{fig:entire-eqnetwork}.
%
\input{floats/eqnetworks}
%
%Many existing methods which aim to increase diversity, require a lot of computational power \citpls{}. We will develop a method that is computationally inexpensive, yet maintains a high diversity in a population. Common methods for measuring the phenotypic diversity of a population only take into account the fitness of each individual \citpls{}. We develop a method for measuring phenotypic diversity that takes into account the different traits among individuals. 
We develop a new method for measuring phenotypic diversity in GAs using neural networks as individuals. We claim that our method better reflects different traits among the individuals. Furthermore, we use our method to explore how different replacement rules affect the diversity of a population.

This paper is organized as follows. \Cref{sec:preliminaries} introduces the concepts used in our diversity measure, and can be skipped if the reader knows about genetic algorithms and neural networks. \Cref{sec:nntd} describes our diversity measure, which is experimentally evaluated and compared to other measures in \cref{sec:experiments}. In \cref{sec:conclusion} we evaluate \dia.

\subsection{Related work}
Concepts and applications of GAs are described in \cite{Cobb93geneticalgorithms,DeJong:1975:ABC:907087,Luke2013Metaheuristics,Syswerda:1989:UCG:645512.657265,ursem2002diversity,fogarty,Whitley:1989:GAS:93126.93169,1250187}. The use of diversity maintenance in GAs is discussed in \cite{diaz2007empirical,Zitzler00comparisonof,Darwen00doesextra,1266373}, and measures of population diversity are described in \cite{Nguyen:2006:ASPGP,simpson1949measurement}. Combining GAs and neural networks is described in \cite{masterThesisGANN}.
