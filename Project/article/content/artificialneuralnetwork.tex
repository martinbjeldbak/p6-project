\subsection{Artificial Neural Network}
An artificial neural network is a finite directed graph that, from the outside, can be seen as a black box, which given the values $x_1, x_2, \dots, x_n$, outputs the values $y_1, y_2, \dots, y_m$. With the right internal structure, a neural network can be used for a variety of applications, e.g.\ recognition of handwriting~\cite{Widrow:1994:NNA:175247.175257}, where the intensities of different pixels in an image are used as input, and a set of output values are produced, where each output could correspond to a symbol. The highest output value gives the prediction of the network.

We now describe the structure and inner workings of neural networks to understand how these output values are calculated based on input values.

\subsubsection{Neurons}
Nodes in the graph of a neural network are called neurons. Three types of neurons exist: input, hidden, and output. Each input neuron receives one of the input values $x_1, x_2, \dots, x_n$ and forwards the same value to each neuron in the first hidden layer. Hidden and output neurons take a number of values as input from edges exiting other neurons, applies a weight to each value, sums them, and then applies a function to produce a single output value. The function applied is called the transfer or activation function, and is the same for all hidden and output neurons in the network. The output value of neuron $i$ is recursively expressed by 
% Note: bias/threshold is not defined below, add if needed
\begin{equation*}\label{eq:weightcalc}
  y_i =
  \begin{cases}
    \var{x}_i                     & \text{if } i \text{ is an input neuron} \\
    \theta\left(\sum_{j=1}^n w_{ji} y_{j}\right) & \text{otherwise}
  \end{cases}
\end{equation*}
%
where $x_i$ is the $i$th input, $n$ is the amount of neurons, $y_i$ is the output value of neuron $i$, $w_{ji}$ is the weight of the edge from neuron $j$ to $i$ ($0$ if no connection exists), and $\theta$ is the activation function, typically defined to be the sigmoid function
%
\begin{equation*}
  \theta\left(t\right) = \frac{1}{1+e^{-t}}
\end{equation*}
%
In a feed-forward network, neurons are placed in one or more layers in an acyclic directed graph, where each neuron in layer $i$ is connected to every neuron in layer $i + 1$. The value(s) output by the last layer, or the output layer, becomes the output of the neural network. \Cref{fig:ann} illustrates the generic graph structure of a feed-forward neural network with a single hidden layer.
%
\begin{figure}[htpb]
  \centering
  \inputresize{drawings/ANN/ANN}
  \caption{Structure of a neural network.}
  \label{fig:ann}
\end{figure}
%
If a neuron is given the value $0$ on all of its inputs, most activation functions will only allow that neuron to output $0$ as well. This is not desirable for all applications, and is often overcome by giving each neuron a bias value. The bias value of a neuron is added to the sum of inputs it receives, before finally applying the activation function. Thus, if a neuron receives only $0$'s as input, it will output the value obtained by applying the activation function to its bias value.

\subsubsection{Training a Neural Network}
Any application of a neural network requires a suitable number of layers, neurons, biases, and weights between neurons, to adequately solve a given problem. How many hidden neurons and layers to have is problem dependent, see~\cite{sarle1997}.

The weights on edges connecting neurons and bias neurons are decided by a process called training. Training processes are categorised into supervised and unsupervised learning. When the desired output of a given problem is known beforehand, supervised learning can be used. An error term is calculated, which affects how the weights between neurons are changed ~\cite{backpropagation}. Consider training an artificially intelligent (AI) player for a computer game. Given a state of the game, it might not be able to tell whether a particular action is good or bad due to the complexity of the game, and hence supervised learning cannot calculate an error term and update the weights accordingly. Instead a genetic algorithm can be used to search for an optimal AI player. The genetic algorithm is an unsupervised learning method.

%A well-known training algorithm called backpropagation, requires that for each input to the neural network, the correct output must already be known. 
%This kind of learning is called supervised learning. For the application of face recognition, this means that the pictures used for training, each has a predicate indicating whether or not it is a picture of a face.

% the backpropagation algorithm cannot calculate an error term and update the weights accordingly. Instead of backpropagation, a genetic algorithm (GA) can be used to search for an optimal AI player. 
%A GA requires that fitness can be measured for each individual. 
%For the application of an AI player, the fitness value of a player indicates how well it performs according to some criteria. This can be measured by simulating the player in the game.
