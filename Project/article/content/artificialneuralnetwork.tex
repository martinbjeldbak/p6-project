\subsection{Artificial Neural Network}
An artificial neural network is a graph structure that can from the outside be seen as a black box, that given the values $x_1, x_2, \dots, x_n$ outputs the values $y_1, y_2, \dots, y_m$. With the right internal structure, a neural network can be used for a variety of purposes, e.g.\ face recognition, where the intensities of different pixels in an image are used as input, and a single output value $y_1$ is produced, where $y_1 = 1$ if the image was of a face and $0$ if not. We now describe the structure and inner workings of neural networks to understand how these output values are calculated based on input values.

\subsubsection{Neurons}
Nodes in the graph of a neural network are called neurons. A neuron takes a number of values as input from edges exiting other neurons, applies a weight to each value, sums them, then finally applies a function to produce a single output value. The function applied is called the transfer function, and is the same for all hidden neurons in the network. This is recursively expressed by the recurrence
% Note: bias/threshold is not defined below, add if needed
\begin{equation}
  y_i =
  \begin{cases}
    \var{input}_i & \text{if } i \text{ is an input neuron} \\
    f\left(\sum_{j=1}^n w_{ji} y_{j} \right) & \text{else} %- \theta_i \right)
  \end{cases}
\end{equation}
where $\var{input}_i$ is the value given to input neuron $i$, $n$ is the amount of neurons, $y_i$ is the output value of neuron $i$, $w_{ji}$ is the weight of the edge from neuron $j$ to $i$ ($0$ if no connection exists), $y_j$ is the output of neuron $j$, and $f$ is the transfer function, usually defined to be the sigmoid function taking the form
\begin{equation*}
  f(t) = \frac{1}{1+e^{-t}}
\end{equation*}

In a feedforward network, neurons are placed in different layers, where each neuron takes the output of all neurons from the previous layer as input. Neurons in the first layer are called input neurons and receive values as input to the neural network.  The neurons in the last layer are called output neurons. The value output by these neurons becomes the output of the neural network. Any neuron in between these layers is called a hidden neuron. \Cref{fig:ann} illustrates the generic graph structure of a feedforward neural network with a single hidden layer.

\begin{figure}[htpb]
  \centering
  \includestandalone[mode=buildnew, width=\linewidth]{drawings/ANN/ANN}
  \caption{Structure of a neural network.}
  \label{fig:ann}
\end{figure}

\subsubsection{Training a neural network}
Any application of a neural network requires that a correct number of layers, neurons, and weights between neurons are found to adequately solve the problem. In many applications, a single layer of $k$ hidden neurons works well, where $k = 0.5\left(\mid\var{inputNeurons}\mid \times \mid\var{outputNeurons}\mid\right)$\citpls. The weights on edges connecting neurons are decided by a process called training. Well known training algorithms, such as backpropagation, typically require that for each input to the neural network, the output is already known. This kind of learning is called supervised learning. For the application of face recognition, this means that the pictures used for training, each has a predicate indicating whether or not it is a picture of a face.\footnote{Should we explain backpropagation more in detail?}

For some purposes, the desired output of a neural network is not known given a number of input values. Consider for instance a computer game, where a player is controlled by a neural network. The neural network takes as input values indicating the state of the game, such as the position of the players and enemies around him. For each input, the neural network returns a single value indicating which action the player should take, e.g.\ move left, move right, or jump. Given a state of the game, we might not be able to say whether an action output by the neural network is right or wrong. It might be wrong to move closer to an enemy if he eliminates you, but it might be right if the next action is successfully to eliminate the enemy.

From this, it is clear that a backpropagation algorithm is not appropriate for training a neural network to control the actions of a artificial intelligent player in a computer game. For this purpose we propose another approach. We can tell how good a neural network performs, or how fit it is, by simulating a game being played using the neural network to control the player and determining the score achieved in the game, or any other function indicating how well the artificial player performed according to some criteria using that particular neural network. By now being able to measure the fitness of a particular neural network, we can use genetic algorithms to create and search for the best performing neural network.

