\subsubsection{Encoding a Neural Network}
%bitstrings
%weight on connections
%each input and each output
%amount of hidden neurons

Neural networks are ideal for decision making and we use them for encoding individuals. Each possible input an individual can get will be recieved through an input neuron. Likewise, each possible action an individual can perform is formulated through the output neurons. The network is constructed with connections between neurons with associated weights. These weights are used to calculate an action given the actual input.

We represent the connections and weights as bitstrings. Each weight is encoded with $n$ bits. Each bitstring is concatenated with the next, to form one large bit string containing the entire encoding for the network.

The bit string is constructed in an ordered manner, such that the first $n$ bits represent the weight for the first connection between the first input neuron and the first hidden neuron, the next $n$ bits represent the weight for the connection between the first input neuron to the second hidden neuron, and so forth. It is this large bit string that is manipulated by crossing different bit strings from different individuals. An illustration of neural networks is presented in \cref{fig:ann}, and should give a good idea of how the bit string is concatenated.

%\[
%  \underbrace{w_{1,1}}_{n} w_{1,2} \ldots w_{i,j}
%\]

%\[
%  \ldots \underbrace{w_{i,j-1}}_{n} w_{i,j} w_{i,j+1} \ldots
%\]

%where $w_{i,j}$ represents the weight of the connection between the $i'th$ and $j'th$ neuron in bits. Each weight is concatenated with the next weight. The lenght of any weight is of size $n$.

If two chromosomes have different bit strings, we say that they have different genotypes. If the behaviour they encode are different, that is, the neural network they encode produces a different output for some input,  we say they have different phenotypes.
