\subsubsection{Neural networks as individuals}
%bitstrings
%weight on connections
%each input and each output
%amount of hidden neurons

Neural networks can be used to solve many types of problems, e.g.\ classification and decision making.
Before we can use neural networks as individuals in a GA, we must define a method for encoding a neural network as a bit string, which will be manipulated by the GA\@. For any GA, we will assume that every individual in its population will have the same architecture. That is, the number of neurons, the size of each layer, and how neurons are connected is the same.

% Uncommented this paragraph friday 2014-3-21, as it just says what a NN is? -Martin
%Each possible input an individual can get will be received through an input neuron. Likewise, each possible action an individual can perform is formulated through the output neurons. The network is constructed with connections between neurons with associated weights. These weights are used to calculate an action given the actual input.

Thus, neural networks differ only in their weights between neurons and the bias of each neuron. Each individual is therefore represented only in terms of the weights and biases. For each GA, any weight and bias is encoded with a fixed number of bits $q$ and $w$, respectively. The bit string is constructed in an ordered manner, such that the first $q$ bits represent the weight of the connection between the first input neuron and the first hidden neuron, the next $q$ bits represent the weight of the connection between the first input neuron and the second hidden neuron, and so forth. \Cref{fig:entire-eqnetwork} shows an example of two neural networks and the bit string that encodes each of them. If biases are used by the GA, these are encoded right after the weights, and ordered such that the first $w$ bits encode the bias of the first neuron, the next $w$ bits encode the bias of the second neuron, and so forth. 
We limit the weights and biases of any neural network to lie in the range $[-5,5]$.
This is because we use the sigmoid activation function, for which the value $f(x)$ changes only a little when $x \leq -5$ and $x  \geq 5$. If a weight or bias of a neural network is encoded by the $u$ bits $b_1 b_2 \cdots b_u$, we decode its real value using the formula
\[
w = (b_12^0 + b_22^1 + \cdots + b_{u-1}2^{u-2})(1-2b_u)\frac{5}{2^u-1}
\]
%\[
%unsigned = b_12^0 + b_22^1 + \cdots + b_{u-1}2^{u-2}\\
%signed = unsigned(1-2b_{u})\\
%weight = 5\frac{signed}{2^u-1}\\
%\]
The first $u-1$ bits represent an increasing sequence of powers of two. The last bit $b_u$ negates the entire value if set (by the factor $(1-2b_u)$). The value is finally normalized to the range $[-5, 5]$ (by the factor $\frac{5}{2^u-1}$).
The number of bits $u$ used to encode a weight or bias is dependent on the problem in question.
If two chromosomes have different bit strings, we say that they have different genotypes. If the neural network they encode produces a different output for some input, we say they have different phenotypes.
% ALso removed this friday 2014-3-21, no need to reference it twice. Possibly if there was an example on how to encode the network in the example?
%An illustration of neural networks is presented in \cref{fig:ann}, and should give a good idea of how the bit string is concatenated.

%\[
%  \underbrace{w_{1,1}}_{n} w_{1,2} \ldots w_{i,j}
%\]

%\[
%  \ldots \underbrace{w_{i,j-1}}_{n} w_{i,j} w_{i,j+1} \ldots
%\]

%where $w_{i,j}$ represents the weight of the connection between the $i'th$ and $j'th$ neuron in bits. Each weight is concatenated with the next weight. The length of any weight is of size $n$.
%
